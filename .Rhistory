}
data <- get_microdata(year = 2019, trimester = 4, type = 'individual')
data <- data %>% filter(REGION == 1, AGLOMERADO %in% c(33))
for(i in seq(60,80,5)) {
adultos_mayores <- data %>% filter(CH06 >= i) %>% select(CODUSU,NRO_HOGAR,COMPONENTE,CH06,REGION,AGLOMERADO)
ninios <- data %>% filter(CH06 %in% 13:17) %>% select(CODUSU,NRO_HOGAR,CH06,REGION,AGLOMERADO) %>% mutate(ninios = TRUE) %>% unique()
match <- adultos_mayores %>% left_join(ninios, by = c('CODUSU','NRO_HOGAR'))
match$ninios[is.na(match$ninios)] <- FALSE
cat('Edad',i,':',mean(match$ninios),'CI:',round(BinomCI(sum(match$ninios),length(match$ninios),conf.level = 0.95)[1,c(2:3)],4),'\n')
}
rm(list = ls())
library(data.table)
personas <- fread('/Users/julianregatky/Desktop/CNPHyV-2010.csv/PERSONA.csv')
head(personas)
summary(personas$P12)
viviendas <- fread('/Users/julianregatky/Desktop/CNPHyV-2010.csv/VIVIENDA.csv')
hogares <- fread('/Users/julianregatky/Desktop/CNPHyV-2010.csv/HOGAR.csv')
head(viviendas)
colnames(personas)
paste0(colnames(hogares),collapse = ',')
paste0(colnames(hogares),collapse = '","')
paste0(colnames(viviendas),collapse = '","')
prov <- fread('/Users/julianregatky/Desktop/CNPHyV-2010.csv/PROV.csv')
paste0(colnames(proc),collapse = '","')
paste0(colnames(prov),collapse = '","')
get_data <- function(ticker) {
start_date = as.Date('1990-01-01')
end_date = Sys.Date()
url = paste0('https://query2.finance.yahoo.com/v8/finance/chart/',ticker,'?formatted=true&interval=1d&period1=',as.numeric(start_date-as.Date('1970-01-01'))*3600*24,'&period2=',as.numeric(end_date-as.Date('1970-01-01'))*3600*24)
quotes = fromJSON(url)
#quotes = content(GET(url,use_proxy(getProxy())),'parse')
return(quotes$chart$result$indicators$adjclose[[1]]$adjclose[[1]])
}
spy <- get_data('SPY')
library(jsonlite)
library(dplyr)
library(nloptr)
spy <- get_data('SPY')
head(spy)
chg <- spy/lag(spy, n=252L)-1
hist(chg)
summary(chg)
sum(spy>-0.45)/sum(spy>-1)
sum(spy>-0.45)
spy[order(spy)]
sum(chg>-0.45)/sum(chg>-1)
sum(chg>-0.45, na.rm = T)/sum(chg>-1, na.rm = T)
0.999098*100
summary(lm(chg ~ lag(chg, n=252L)))
chg[length(chg)]
0.05985596*0.134988+0.096543
summary(lm(chg ~ lag(chg, n=252L) + I(lag(chg, n=252L)^2)))
(0.05985596^2)*0.928364+0.05985596*0.044499+0.068804
(0.05985596^2)*0.928364+0.05985596*0.044499+0.068804-0.1635*1.96
summary(lm(chg ~ 0))
summary(lm(chg ~ 1))
0.1675*1.96
0.108456-0.1675*1.96
sum(chg < -0.45, na.rm = TRUE)
head(chg)
length(chg)
6904/252
library(dplyr)
table <- read.csv('/Users/julianregatky/Downloads/Macro MPP TP1 - Hoja 1.csv')
head(table)
for(i in 1:nrow(table)) {
cat(table$Nombre[i],':',table$Nota[i],'\n',table$Ejercicio.1,'\n',table$Ejercicio.2,'\n',table$Ejercicio.3,'\n','-----------------------------')
}
for(i in 1:nrow(table)) {
cat(table$Nombre[i],':',table$Nota[i],'\n',table$Ejercicio.1[i],'\n',table$Ejercicio.2[i],'\n',table$Ejercicio.3[i],'\n','-----------------------------')
}
for(i in 1:nrow(table)) {
cat(table$Nombre[i],':',table$Nota[i],'\n',table$Ejercicio.1[i],'\n',table$Ejercicio.2[i],'\n',table$Ejercicio.3[i],'\n\n','-----------------------------\n')
}
head(table)
head(table$Nombre)
i = 1
table$Nombre[i]
for(i in 1:nrow(table)) {
cat(as.character(table$Nombre[i]),':',table$Nota[i],'\n',table$Ejercicio.1[i],'\n',table$Ejercicio.2[i],'\n',table$Ejercicio.3[i],'\n\n','-----------------------------\n')
}
table <- read.csv('/Users/julianregatky/Downloads/Macro MPP TP1 - Hoja 1.csv', stringsAsFactors = FALSE)
for(i in 1:nrow(table)) {
cat(as.character(table$Nombre[i]),':',table$Nota[i],'\n',table$Ejercicio.1[i],'\n',table$Ejercicio.2[i],'\n',table$Ejercicio.3[i],'\n\n','-----------------------------\n')
}
for(i in 1:nrow(table)) {
cat(as.character(table$Nombre[i]),':',table$Nota[i],'\nEjercicio 1)',table$Ejercicio.1[i],'\nEjercicio 2)',table$Ejercicio.2[i],'\nEjercicio 3)',table$Ejercicio.3[i],'\n\n','-----------------------------\n')
}
library(jsonlite)
library(dplyr)
library(nloptr)
get_data <- function(ticker) {
start_date = as.Date('2005-01-01')
end_date = Sys.Date()
url = paste0('https://query2.finance.yahoo.com/v8/finance/chart/',ticker,'?formatted=true&interval=1mo&period1=',as.numeric(start_date-as.Date('1970-01-01'))*3600*24,'&period2=',as.numeric(end_date-as.Date('1970-01-01'))*3600*24)
quotes = fromJSON(url)
#quotes = content(GET(url,use_proxy(getProxy())),'parse')
return(quotes$chart$result$indicators$adjclose[[1]]$adjclose[[1]])
}
tickers = c('GLD','XLP','XLY','TLT','IEF','XLF','VNQ','EEM','SHY','SPY','LQD','XLU','XLK','IJH','IWM')
data = suppressMessages(lapply(tickers, get_data) %>% bind_cols())
colnames(data) = tickers
head(tickers)
head(data)
x <<- matrix(as.numeric(apply(data,2,function(x) return(x/lag(x)-1)) %>% na.omit()),ncol=length(tickers))
get_data <- function(ticker) {
start_date = as.Date('2005-01-01')
end_date = Sys.Date()
url = paste0('https://query2.finance.yahoo.com/v8/finance/chart/',ticker,'?formatted=true&interval=1wk&period1=',as.numeric(start_date-as.Date('1970-01-01'))*3600*24,'&period2=',as.numeric(end_date-as.Date('1970-01-01'))*3600*24)
quotes = fromJSON(url)
#quotes = content(GET(url,use_proxy(getProxy())),'parse')
return(quotes$chart$result$indicators$adjclose[[1]]$adjclose[[1]])
}
tickers = c('GLD','XLP','XLY','TLT','IEF','XLF','VNQ','EEM','SHY','SPY','LQD','XLU','XLK','IJH','IWM')
data = suppressMessages(lapply(tickers, get_data) %>% bind_cols())
colnames(data) = tickers
x <<- matrix(as.numeric(apply(data,2,function(x) return(x/lag(x)-1)) %>% na.omit()),ncol=length(tickers))
z <<- as.factor(rep_len(1:10,nrow(x)) %>% .[order(.)])
obj <- function(b) {
u_sq = (x %*% b - mean(x %*% b))^2
model = lm(u_sq ~ z)
return(as.numeric(summary(model)$fstatistic[1]))
}
S <- slsqp(runif(ncol(x)), fn = obj,
lower = rep(0,ncol(x)))
print(S$message)
print(S$value)
data.frame(symbol = tickers,
weight = round(S$par/sum(S$par),2))
rtn <- x %*% round(S$par/sum(S$par),2)
y = rtn
y = log(y^2)
model <- lm(y ~ z)
fitted = predict(model,as.data.frame(z))
plot(y)
lines(x = 1:length(fitted), y = fitted, col = "red", lwd = 2)
get_data <- function(ticker) {
start_date = as.Date('2005-01-01')
end_date = Sys.Date()
url = paste0('https://query2.finance.yahoo.com/v8/finance/chart/',ticker,'?formatted=true&interval=1mo&period1=',as.numeric(start_date-as.Date('1970-01-01'))*3600*24,'&period2=',as.numeric(end_date-as.Date('1970-01-01'))*3600*24)
quotes = fromJSON(url)
#quotes = content(GET(url,use_proxy(getProxy())),'parse')
return(quotes$chart$result$indicators$adjclose[[1]]$adjclose[[1]])
}
tickers = c('GLD','XLP','XLY','TLT','IEF','XLF','VNQ','EEM','SHY','SPY','LQD','XLU','XLK','IJH','IWM')
data = suppressMessages(lapply(tickers, get_data) %>% bind_cols())
colnames(data) = tickers
x <<- matrix(as.numeric(apply(data,2,function(x) return(x/lag(x)-1)) %>% na.omit()),ncol=length(tickers))
z <<- as.factor(rep_len(1:10,nrow(x)) %>% .[order(.)])
obj <- function(b) {
u_sq = (x %*% b - mean(x %*% b))^2
model = lm(u_sq ~ z)
return(as.numeric(summary(model)$fstatistic[1]))
}
S <- slsqp(runif(ncol(x)), fn = obj,
lower = rep(0,ncol(x)))
print(S$message)
print(S$value)
data.frame(symbol = tickers,
weight = round(S$par/sum(S$par),2))
rtn <- x %*% round(S$par/sum(S$par),2)
y = rtn
y = log(y^2)
model <- lm(y ~ z)
fitted = predict(model,as.data.frame(z))
plot(y)
lines(x = 1:length(fitted), y = fitted, col = "red", lwd = 2)
get_data <- function(ticker) {
start_date = as.Date('2005-01-01')
end_date = Sys.Date()
url = paste0('https://query2.finance.yahoo.com/v8/finance/chart/',ticker,'?formatted=true&interval=1wk&period1=',as.numeric(start_date-as.Date('1970-01-01'))*3600*24,'&period2=',as.numeric(end_date-as.Date('1970-01-01'))*3600*24)
quotes = fromJSON(url)
#quotes = content(GET(url,use_proxy(getProxy())),'parse')
return(quotes$chart$result$indicators$adjclose[[1]]$adjclose[[1]])
}
tickers = c('GLD','XLP','XLY','TLT','IEF','XLF','VNQ','EEM','SHY','SPY','LQD','XLU','XLK','IJH','IWM')
data = suppressMessages(lapply(tickers, get_data) %>% bind_cols())
colnames(data) = tickers
x <<- matrix(as.numeric(apply(data,2,function(x) return(x/lag(x)-1)) %>% na.omit()),ncol=length(tickers))
z <<- as.factor(rep_len(1:10,nrow(x)) %>% .[order(.)])
obj <- function(b) {
u_sq = (x %*% b - mean(x %*% b))^2
model = lm(u_sq ~ z)
return(as.numeric(summary(model)$fstatistic[1]))
}
S <- slsqp(runif(ncol(x)), fn = obj,
lower = rep(0,ncol(x)))
print(S$message)
print(S$value)
data.frame(symbol = tickers,
weight = round(S$par/sum(S$par),2))
rtn <- x %*% round(S$par/sum(S$par),2)
b <- round(S$par/sum(S$par),2)
u_sq = (x %*% b - mean(x %*% b))^2
model = lm(u_sq ~ z)
summary(model)
rm(list=ls())
# Cargamos todas las librerías necesarias
# pacman las carga y, de no estar instaladas, previamente las instala
if (!require('pacman')) install.packages('pacman')
pacman::p_load(tidyverse,mlr,glmnet,ROCR,splines,rpart,randomForest,caret)
# Fijamos el working directory
setwd('/Users/julianregatky/Documents/GitHub/client_churn_ML2020')
# Importamos el dataset
dataset <- read.table('train.csv', header = T, sep =',', dec = '.')
# Cargamos funciones propias
source('functions.R')
# Eliminamos ID
dataset <- dataset %>% select(-ID)
# Eliminamos features cuasi-constantes
dataset <- removeConstantFeatures(dataset,
perc = 0.01, # Fijamos threshold del 99%
dont.rm = 'TARGET')
# Unificamos features duplicados
del_col <- c()
for(i in 1:(ncol(dataset)-1)) {
for(j in (i+1):ncol(dataset)) {
if(identical(dataset[,i],dataset[,j])) {
# identificamos columnas idénticas en el data.frame
del_col <- c(del_col,j)
}
}
}
dataset <- dataset[,setdiff(1:ncol(dataset),del_col)] # Eliminamos una de las duplicadas
# La variable 'nac' es la única que cuenta con datos faltantes (56 obs con NA)
# Esto representa sólo ~0.17% de las obs. Imputamos datos faltantes con el valor 2
# que es el reportado para la variable en ~97.8% de los casos.
sum(is.na(dataset$nac))/length(dataset$nac) # ~0.17%
sum(dataset$nac[!is.na(dataset$nac)] == 2)/sum(!is.na(dataset$nac)) # ~97.8%
dataset$nac[is.na(dataset$nac)] <- 2
set.seed(123) # Por replicabilidad
index_train <- sample(1:nrow(dataset),round(nrow(dataset)*0.9)) # Muestra de training
train <- dataset[index_train,]
validation <- dataset[setdiff(1:nrow(dataset),index_train),]
mtry <- sqrt(ncol(train))
tunegrid <- expand.grid(.mtry=mtry)
tunegrid
control <- trainControl(method='repeatedcv',
number=10,
repeats=3,
search='grid')
control
rm(list=ls())
# Cargamos todas las librerías necesarias
# pacman las carga y, de no estar instaladas, previamente las instala
if (!require('pacman')) install.packages('pacman')
pacman::p_load(tidyverse,mlr,glmnet,ROCR,splines,rpart,randomForest)
# Fijamos el working directory
setwd('/Users/julianregatky/Documents/GitHub/client_churn_ML2020')
# Importamos el dataset
dataset <- read.table('train.csv', header = T, sep =',', dec = '.')
# Cargamos funciones propias
source('functions.R')
# Eliminamos ID
dataset <- dataset %>% select(-ID)
# Eliminamos features cuasi-constantes
dataset <- removeConstantFeatures(dataset,
perc = 0.01, # Fijamos threshold del 99%
dont.rm = 'TARGET')
# Unificamos features duplicados
del_col <- c()
for(i in 1:(ncol(dataset)-1)) {
for(j in (i+1):ncol(dataset)) {
if(identical(dataset[,i],dataset[,j])) {
# identificamos columnas idénticas en el data.frame
del_col <- c(del_col,j)
}
}
}
dataset <- dataset[,setdiff(1:ncol(dataset),del_col)] # Eliminamos una de las duplicadas
# La variable 'nac' es la única que cuenta con datos faltantes (56 obs con NA)
# Esto representa sólo ~0.17% de las obs. Imputamos datos faltantes con el valor 2
# que es el reportado para la variable en ~97.8% de los casos.
sum(is.na(dataset$nac))/length(dataset$nac) # ~0.17%
sum(dataset$nac[!is.na(dataset$nac)] == 2)/sum(!is.na(dataset$nac)) # ~97.8%
dataset$nac[is.na(dataset$nac)] <- 2
set.seed(123) # Por replicabilidad
index_train <- sample(1:nrow(dataset),round(nrow(dataset)*0.9)) # Muestra de training
# Si agregamos nomonios adicionales para todos los features
# el algoritmo que usa glmnet para la regresión no converge
# Seleccionamos las variables para splines con un árbol de decisión
tree <- rpart(formula = TARGET ~., data = dataset)
features_importantes <- names(tree$variable.importance)
# Control local polinómico de hasta grado 3
features.spline <- splines_matrix(dataset[,features_importantes])
# Agregamos términos polinómicos para el resto de las variables
features.polynomial <- polynomial(dataset[,setdiff(colnames(dataset),c(features_importantes,'TARGET'))],
n = 2)
dataset.lasso <- cbind(dataset,features.spline,features.polynomial)
x_train <- dataset.lasso[index_train,] %>% select(-TARGET)
x_validation <- dataset.lasso[setdiff(1:nrow(dataset.lasso),index_train),] %>% select(-TARGET)
y_train <- dataset.lasso[index_train,'TARGET']
y_validation <- dataset.lasso[setdiff(1:nrow(dataset.lasso),index_train),'TARGET']
# Cross-Validation, 10 folds
x_train_matrix <- model.matrix( ~ .-1, x_train)
cv.out = cv.glmnet(x_train_matrix, y_train, alpha = 1, nfolds = 10)
plot(cv.out)
lambda_star = cv.out$lambda.min
# Estimamos el modelo con el lambda de CV
model.lasso = glmnet(x = x_train_matrix,
y = y_train,
family = 'binomial',
alpha = 1, # LASSO
lambda = lambda_star,
standardize = TRUE)
x_validation_matrix <- model.matrix( ~ .-1, x_validation)
pred.lasso = predict(model.lasso, s = lambda_star , newx = x_validation_matrix, type = 'response')
performance(prediction(pred.lasso,y_validation),"auc")@y.values[[1]] #AUC
auc_lasso <- performance(prediction(pred.lasso,y_validation),"tpr","fpr")
plot(auc_lasso)
train <- dataset[index_train,]
validation <- dataset[setdiff(1:nrow(dataset),index_train),]
random.forest = randomForest(TARGET ~ .,
data = train,
mtry = floor(sqrt(ncol(train))), # m = 10
ntree = 1000,                  # B
sample = 0.5*floor(train), # tamaño de cada re-muestra bootstrap.
maxnodes = 10,  # cantidad máxima de nodos terminales en c/ árbol.
nodesize = 150, # cantidad mínima de datos en nodo terminal.
importance = T,   # Computar importancia de c/ covariable.
proximity = F    #computa la matriz de proximidad entre observaciones.
)
pred.rforest = predict(random.forest,newdata=validation)
performance(prediction(pred.rforest,validation$TARGET),"auc")@y.values[[1]] #AUC
auc_rforest <- performance(prediction(pred.rforest,y_validation),"tpr","fpr")
points(auc_rforest@x.values[[1]],auc_rforest@y.values[[1]], type = 'l', col = 'red')
floor(sqrt(ncol(train)))
floor(train)
full_grid <- expand.grid(mtry = 5:20, sample = seq(0.4,0.8,0.1), maxnodes = 20:50, nodesize = 50:200)
random_grid <- full_grid[sample(1:nrow(full_grid),30),]
random_grid
floor(sqrt(ncol(train)))
full_grid <- expand.grid(mtry = 5:20, sample = seq(0.4,0.8,0.1), maxnodes = 20:50, nodesize = 50:200)
random_grid <- full_grid[sample(1:nrow(full_grid),30),]
paste(colnames(random_grid),random_grid[1])
paste(colnames(random_grid),random_grid[1,])
paste(colnames(random_grid),random_grid[1,],collapse = ':')
paste(colnames(random_grid),random_grid[1,],collapse = ' - ')
i = 1
cat(i,'-',paste(colnames(random_grid),random_grid[1,],collapse = ' - '))
cat(i,'|',paste(colnames(random_grid),random_grid[1,],collapse = ' - '))
pred.rforest = predict(random.forest,newdata=validation)
performance(prediction(pred.rforest,validation$TARGET),"auc")@y.values[[1]]
cat(i,'|',paste(colnames(random_grid),random_grid[1,],collapse = ' - '),'| auc:',performance(prediction(pred.rforest,validation$TARGET),"auc")@y.values[[1]])
full_grid <- expand.grid(mtry = 5:20, sample = seq(0.4,0.8,0.1), maxnodes = 20:50, nodesize = 50:200)
random_grid <- full_grid[sample(1:nrow(full_grid),30),]
best_auc <- 0
for(i in 1:nrow(random_grid)) {
random.forest <- randomForest(TARGET ~ .,
data = train,
mtry = random_grid$mtry[i],
ntree = 10,
sample = floor(random_grid$sample[i]*nrow(train)),
maxnodes = random_grid$maxnodes[i],
nodesize = random_grid$nodesize[i],
importance = T,
proximity = F
)
pred.rforest = predict(random.forest,newdata=validation)
cat(i,'|',paste(colnames(random_grid),random_grid[1,],collapse = ' - '),'| auc:',performance(prediction(pred.rforest,validation$TARGET),"auc")@y.values[[1]])
if(performance(prediction(pred.rforest,validation$TARGET),"auc")@y.values[[1]] > best_auc) {
best_model <- random.forest
}
}
full_grid <- expand.grid(mtry = 5:20, sample = seq(0.4,0.8,0.1), maxnodes = 20:50, nodesize = 50:200)
random_grid <- full_grid[sample(1:nrow(full_grid),30),]
best_auc <- 0
for(i in 1:nrow(random_grid)) {
random.forest <- randomForest(TARGET ~ .,
data = train,
mtry = random_grid$mtry[i],
ntree = 10,
sample = floor(random_grid$sample[i]*nrow(train)),
maxnodes = random_grid$maxnodes[i],
nodesize = random_grid$nodesize[i],
importance = T,
proximity = F
)
pred.rforest = predict(random.forest,newdata=validation)
cat(i,'|',paste(colnames(random_grid),random_grid[1,],collapse = ' - '),'| auc:',performance(prediction(pred.rforest,validation$TARGET),"auc")@y.values[[1]],'\n')
if(performance(prediction(pred.rforest,validation$TARGET),"auc")@y.values[[1]] > best_auc) {
best_model <- random.forest
}
}
index_validation <- sample(index_train,round(length(index_train)*0.2))
index_validation <- sample(index_train,round(length(index_train)*0.2))
train <- dataset[setdiff(index_train, index_validation),]
validation <- dataset[index_validation,]
test <- dataset[setdiff(1:nrow(dataset),index_train),]
full_grid <- expand.grid(mtry = 5:20, sample = seq(0.4,0.8,0.1), maxnodes = 20:50, nodesize = 50:200)
random_grid <- full_grid[sample(1:nrow(full_grid),30),]
best_auc <- 0
index_validation <- sample(index_train,round(length(index_train)*0.2))
train <- dataset[setdiff(index_train, index_validation),]
validation <- dataset[index_validation,]
test <- dataset[setdiff(1:nrow(dataset),index_train),]
full_grid <- expand.grid(mtry = 5:20, sample = seq(0.4,0.8,0.1), maxnodes = 20:50, nodesize = 50:200)
random_grid <- full_grid[sample(1:nrow(full_grid),30),]
best_auc <- 0
for(i in 1:nrow(random_grid)) {
random.forest <- randomForest(TARGET ~ .,
data = train,
mtry = random_grid$mtry[i],
ntree = 10,
sample = floor(random_grid$sample[i]*nrow(train)),
maxnodes = random_grid$maxnodes[i],
nodesize = random_grid$nodesize[i],
importance = T,
proximity = F
)
pred.rforest = predict(random.forest,newdata=validation)
cat(i,'|',paste(colnames(random_grid),random_grid[1,],collapse = ' - '),'| auc:',performance(prediction(pred.rforest,validation$TARGET),"auc")@y.values[[1]],'\n')
if(performance(prediction(pred.rforest,validation$TARGET),"auc")@y.values[[1]] > best_auc) {
best_model <- random.forest
}
}
pred.rforest = predict(best_model,newdata=test)
performance(prediction(pred.rforest,validation$TARGET),"auc")@y.values[[1]] #AUC
performance(prediction(pred.rforest,test$TARGET),"auc")@y.values[[1]] #AUC
auc_rforest <- performance(prediction(pred.rforest,y_validation),"tpr","fpr")
points(auc_rforest@x.values[[1]],auc_rforest@y.values[[1]], type = 'l', col = 'red')
dev.off()
full_grid <- expand.grid(mtry = 5:20, sample = seq(0.4,0.8,0.1), maxnodes = 20:50, nodesize = 50:200)
random_grid <- full_grid[sample(1:nrow(full_grid),30),]
best_auc <- 0
for(i in 1:nrow(random_grid)) {
random.forest <- randomForest(TARGET ~ .,
data = train,
mtry = random_grid$mtry[i],
ntree = 20,
sample = floor(random_grid$sample[i]*nrow(train)),
maxnodes = random_grid$maxnodes[i],
nodesize = random_grid$nodesize[i],
importance = T,
proximity = F
)
pred.rforest = predict(random.forest,newdata=validation)
cat(i,'|',paste(colnames(random_grid),random_grid[1,],collapse = ' - '),'| auc:',performance(prediction(pred.rforest,validation$TARGET),"auc")@y.values[[1]],'\n')
if(performance(prediction(pred.rforest,validation$TARGET),"auc")@y.values[[1]] > best_auc) {
best_model <- random.forest
}
}
full_grid <- expand.grid(mtry = 5:20, sample = seq(0.4,0.8,0.1), maxnodes = 20:50, nodesize = 50:200)
random_grid <- full_grid[sample(1:nrow(full_grid),30),]
best_auc <- 0
for(i in 1:nrow(random_grid)) {
random.forest <- randomForest(TARGET ~ .,
data = train,
mtry = random_grid$mtry[i],
ntree = 20,
sample = floor(random_grid$sample[i]*nrow(train)),
maxnodes = random_grid$maxnodes[i],
nodesize = random_grid$nodesize[i],
importance = T,
proximity = F
)
pred.rforest = predict(random.forest,newdata=validation)
cat(i,'|',paste(colnames(random_grid),random_grid[i,],collapse = ' - '),'| auc:',performance(prediction(pred.rforest,validation$TARGET),"auc")@y.values[[1]],'\n')
if(performance(prediction(pred.rforest,validation$TARGET),"auc")@y.values[[1]] > best_auc) {
best_model <- random.forest
}
}
pred.rforest = predict(best_model,newdata=test)
performance(prediction(pred.rforest,test$TARGET),"auc")@y.values[[1]] #AUC
auc_rforest <- performance(prediction(pred.rforest,y_validation),"tpr","fpr")
plot(auc_lasso)
function (x, ...)
points(auc_rforest@x.values[[1]],auc_rforest@y.values[[1]], type = 'l', col = 'red')
pred.rforest = predict(best_model,newdata=test)
performance(prediction(pred.rforest,test$TARGET),"auc")@y.values[[1]] #AUC
auc_rforest <- performance(prediction(pred.rforest,y_validation),"tpr","fpr")
points(auc_rforest@x.values[[1]],auc_rforest@y.values[[1]], type = 'l', col = 'red')
pacman::p_load(tidyverse,mlr,glmnet,ROCR,splines,rpart,randomForest,gbm)
full_grid <- expand.grid(mtry = 5:20, sample = seq(0.4,0.8,0.1), maxnodes = 20:50, nodesize = 50:200, ntree = 500:1500)
random_grid <- full_grid[sample(1:nrow(full_grid),30),]
full_grid <- expand.grid(mtry = 5:20, sample = seq(0.4,0.8,0.1), maxnodes = 20:50, nodesize = 50:200, ntree = seq(500,1500,100))
random_grid <- full_grid[sample(1:nrow(full_grid),30),]
full_grid <- expand.grid(n.trees = seq(500,5000,500), shrinkage = seq(0.001,0.01,0.001), interaction.depth = 2:10, train.fraction = seq(0.5,0.9,0.1), bag.fraction = seq(0.5,0.9,0.1))
random_grid <- full_grid[sample(1:nrow(full_grid),30),]
head(random_grid)
full_grid <- expand.grid(n.trees = seq(500,5000,200), shrinkage = seq(0.001,0.01,0.001), interaction.depth = 2:10, train.fraction = seq(0.5,0.9,0.1), bag.fraction = seq(0.5,0.9,0.1))
full_grid <- expand.grid(n.trees = seq(500,5000,100), shrinkage = seq(0.001,0.01,0.001), interaction.depth = 2:10, train.fraction = seq(0.5,0.9,0.1), bag.fraction = seq(0.5,0.9,0.1))
random_grid <- full_grid[sample(1:nrow(full_grid),30),]
head(random_grid)
hist(random_grid$n.trees)
summary(random_grid$n.trees)
random_grid$n.trees <- rep(10,nrow(random_grid))
best_auc <- 0
for(i in 1:nrow(random_grid)) {
model.gbm = gbm(TARGET ~ .,
data = train,
distribution = "bernoulli",
n.trees = random_grid$n.trees[i],
shrinkage = random_grid$shrinkage[i],
interaction.depth = random_grid$interaction.depth[i],
train.fraction = random_grid$train.fraction[i],
bag.fraction = random_grid$bag.fraction[i],
cv.folds = 5,
verbose = F)
pred.gbm = predict(model.gbm,newdata=validation)
cat(i,'|',paste(colnames(random_grid),random_grid[i,],collapse = ' - '),'| auc:',performance(prediction(pred.gbm,validation$TARGET),"auc")@y.values[[1]],'\n')
if(performance(prediction(pred.gbm,validation$TARGET),"auc")@y.values[[1]] > best_auc) {
best_model <- model.gbm
}
}
pred.rforest = predict(best_model,newdata=test)
performance(prediction(pred.rforest,test$TARGET),"auc")@y.values[[1]] #AUC
auc_rforest <- performance(prediction(pred.rforest,y_validation),"tpr","fpr")
points(auc_rforest@x.values[[1]],auc_rforest@y.values[[1]], type = 'l', col = 'red')
pred.rforest = pred.outsample.gbm(best_model,newdata=test)
pred.rforest = predict(best_model,newdata=test)
performance(prediction(pred.rforest,test$TARGET),"auc")@y.values[[1]] #AUC
