ingreso <- c()
for(arribo in arribos) {
ing <- 1648.96*min(round(arribo*0.85),i)+(100-i)*1498.96
ingreso <- c(ingreso,ing)
}
cat('\r',i)
y <- c(y,mean(ingreso))
}
arribos <- sample(data$mesas_ocupadas,5000,replace = T)
y <- c()
for(i in 0:100) {
ingreso <- c()
for(arribo in arribos) {
ing <- 1648.96*min(round(arribo*0.85),i)+(100-i)*1498.96
ingreso <- c(ingreso,ing)
}
cat('\r',i)
y <- c(y,mean(ingreso))
}
data.frame(0:100,y)
i = 10
ingreso <- c()
for(arribo in arribos) {
ing <- 1648.96*min(round(arribo*0.85),i)+(100-i)*1498.96
ingreso <- c(ingreso,ing)
}
summary(ingreso)
sd(ingreso)
S = expand.grid(1:30,1:30)
S
library(readxl)
rm(list = ls())
if(!require('pacman')) install.packages('pacman')
pacman::p_load(readxl,dplyr)
data <- read_excel(path = '/Users/julianregatky/Documents/GitHub/MiM/LyO/TP2/AirlineCallCenterData.xlsx',
sheet = 1,
skip = 3,
col_names = c('client','arrival','client_type','agent_type','processing_time'))
library(data.table)
data <- fread('/Users/julianregatky/Desktop/34_1110834_compressed_loan.csv.zip')
data <- fread('/Users/julianregatky/Desktop/loan.csv')
head(data)
str(data)
issue_d <- unique(data$issue_d)
data <- as.data.frame(data)
i = 1
head(data[,i])
library(data.table)
data <- fread('/Users/julianregatky/Desktop/loan.csv')
summary(data)
sakasegawa <- function(S,mu,lambda,cv_a,cv_s) {
rho <- min(lambda/(S*mu),1)
tiempo_ciclo <- 1/(S*mu)
factor_utilizacion <- (rho^(sqrt(2*(S+1))-1))/(1-rho)
factor_variablidad <- (cv_a^2+cv_s^2)/2
return(tiempo_ciclo*factor_utilizacion*factor_variablidad)
}
sakasegawa <- function(S,mu,lambda,cv_a,cv_s) {
rho <- min(lambda/(S*mu),1)
tiempo_ciclo <- 1/(S*mu)
factor_utilizacion <- (rho^(sqrt(2*(S+1))-1))/(1-rho)
factor_variablidad <- (cv_a^2+cv_s^2)/2
return(tiempo_ciclo*factor_utilizacion*factor_variablidad)
}
tabla <- c()
for(S in 1:10) {
tabla <- c(S,sakasegawa(S,12,10,1,0.8))
}
tabla <- as.data.frame(matrix(tabla,ncol=2))
tabla <- c()
for(S in 1:10) {
tabla <- c(S,sakasegawa(S,12,10,1,0.8))
}
tabla <- as.data.frame(matrix(tabla,ncol=2,byrow = TRUE))
colnames(tabla) <- c('S','W_q')
tabla
tabla <- c()
for(S in 1:10) {
tabla <- c(tabla,S,sakasegawa(S,12,10,1,0.8))
}
tabla <- as.data.frame(matrix(tabla,ncol=2,byrow = TRUE))
colnames(tabla) <- c('S','W_q')
tabla
tabla <- c()
for(S in 1:10) {
tabla <- c(tabla,S,round(sakasegawa(S,12,10,1,0.8),2))
}
tabla <- as.data.frame(matrix(tabla,ncol=2,byrow = TRUE))
colnames(tabla) <- c('S','W_q')
tabla
tabla <- c()
for(S in 1:10) {
tabla <- c(tabla,S,round(sakasegawa(S,12,10,1,0.8),4))
}
tabla <- as.data.frame(matrix(tabla,ncol=2,byrow = TRUE))
colnames(tabla) <- c('S','W_q')
tabla
0.0165*60
sakasegawa <- function(S,mu,lambda,cv_a,cv_s) {
rho <- min(lambda/(S*mu),1)
tiempo_ciclo <- 1/(S*mu)
factor_utilizacion <- (rho^(sqrt(2*(S+1))-1))/(1-rho)
factor_variablidad <- (cv_a^2+cv_s^2)/2
return(tiempo_ciclo*factor_utilizacion*factor_variablidad)
}
sakasegawa(1,12,10,1,0.8)
sakasegawa(1,12,10,1,0.8)*60
sakasegawa(1,12,10,1,0.1)*60
qnorm(0.9,60,7)
1-qnorm(0.9,60,7)
qnorm(0.1,60,7)
qnorm(0.99999999999,60,7)-qnorm(0.9,60,7)
qnorm(0.9999999999999999999,60,7)-qnorm(0.9,60,7)
qnorm(0.99999999999999999,60,7)-qnorm(0.9,60,7)
qnorm(0.9999999999999,60,7)-qnorm(0.9,60,7)
qnorm(0.9,60,7)
qnorm(1/3,25,15)
qnorm(0.9,48000,250*48)
rep_len(1:10,11)
shuffle(rep_len(1:10,25))
rep_len(1:10,25)
sample(rep_len(1:10,25))
sakasegawa <- function(S,mu,lambda,cv_a,cv_s) {
rho <- min(lambda/(S*mu),1)
tiempo_ciclo <- 1/(S*mu)
factor_utilizacion <- (rho^(sqrt(2*(S+1))-1))/(1-rho)
factor_variablidad <- (cv_a^2+cv_s^2)/2
return(tiempo_ciclo*factor_utilizacion*factor_variablidad)
}
sasakasegawa(3,0.25,0.5,1,0.5)
sakasegawa(3,0.25,0.5,1,0.5)
sakasegawa(3,0.25,0.5,1,0.5)*0.5
library(jsonlite)
library(dplyr)
library(nloptr)
get_data <- function(ticker) {
start_date = as.Date('2005-01-01')
end_date = Sys.Date()
url = paste0('https://query2.finance.yahoo.com/v8/finance/chart/',ticker,'?formatted=true&interval=1d&period1=',as.numeric(start_date-as.Date('1970-01-01'))*3600*24,'&period2=',as.numeric(end_date-as.Date('1970-01-01'))*3600*24)
quotes = fromJSON(url)
#quotes = content(GET(url,use_proxy(getProxy())),'parse')
return(quotes$chart$result$indicators$adjclose[[1]]$adjclose[[1]])
}
tickers = c('GLD','XLP','XLY','TLT','IEF','XLF','VNQ','EEM','SHY','SPY','LQD','XLU','XLK','IJH','IWM')
data = suppressMessages(lapply(tickers, get_data) %>% bind_cols())
colnames(data) = tickers
x <<- matrix(as.numeric(apply(data,2,function(x) return(x/lag(x)-1)) %>% na.omit()),ncol=length(tickers))
z <<- as.factor(rep_len(1:10,nrow(x)) %>% .[order(.)])
obj <- function(b) {
u_sq = (x %*% b - mean(x %*% b))^2
model = lm(u_sq ~ z)
return(as.numeric(summary(model)$fstatistic[1]))
}
S <- slsqp(runif(ncol(x)), fn = obj,
lower = rep(0,ncol(x)))
print(S$message)
print(S$value)
data.frame(symbol = tickers,
weight = round(S$par/sum(S$par),2))
profit_l <- rnorm(1000,1000,300)*(100-43)
profit_l <- mean(rnorm(1000,1000,300)*(100-43))
Q_l <- (100-43)/43
Q_l <- (100-43)/100
Q_l <- (100-43)/100
Q_l <- qnorm((100-43)/100,1000,300)
profit_l <- mean(min(rnorm(1000,1000,300),Q_l)*(100-43))
min(rnorm(1000,1000,300),Q_l)
min(rnorm(1000,1000,300),Q_l)
min(rnorm(1000,1000,300),rep(Q_l,1000))
Q_l <- qnorm((100-43)/100,1000,300)
Q_l <- qnorm((100-43)/100,1000,300)
profit_l <- c()
for(i in 1:1000) {
profit_l <- c(profit_l,min(rnorm(1,1000,300),Q_l)*(100-43))
}
mean(profit_l)
Q_l <- qnorm((100-43)/100,1000,300)
profit_l <- c()
for(i in 1:1000) {
profit_l <- c(profit_l,min(rnorm(1,1000,300),Q_l)-Q_l*43)
}
mean(profit_l)
Q_l <- qnorm((100-43)/100,1000,300)
profit_l <- c()
for(i in 1:1000) {
profit_l <- c(profit_l,min(rnorm(1,1000,300),Q_l)-Q_l*43)
}
mean(profit_l)
Q_l <- qnorm((100-43)/100,1000,300)
profit_l <- c()
for(i in 1:1000) {
profit_l <- c(profit_l,min(rnorm(1,1000,300),Q_l)*100-Q_l*43)
}
mean(profit_l)
Q_l <- qnorm((100-43)/100,1000,300)
profit_l <- c()
for(i in 1:1000) {
profit_l <- c(profit_l,min(rnorm(1,1000,300),Q_l)*100-Q_l*43)
}
mean(profit_l)
Q_l <- qnorm((100-43)/100,1000,300)
profit_l <- c()
for(i in 1:5000) {
profit_l <- c(profit_l,min(rnorm(1,1000,300),Q_l)*100-Q_l*43)
}
mean(profit_l)
Q_l <- qnorm((100-43)/100,1000,300)
profit_l <- c()
for(i in 1:5000) {
profit_l <- c(profit_l,min(rnorm(1,1000,300),Q_l)*100-Q_l*43)
}
mean(profit_l)
Q_l <- qnorm((100-43)/100,1000,300)
profit_l <- c()
for(i in 1:5000) {
profit_l <- c(profit_l,min(rnorm(1,1000,300),Q_l)*100-Q_l*43)
}
mean(profit_l)
Q_l <- qnorm((100-43)/100,1000,300)
profit_l <- c()
for(i in 1:10000) {
profit_l <- c(profit_l,min(rnorm(1,1000,300),Q_l)*100-Q_l*43)
}
mean(profit_l)
Q_l <- qnorm((100-43)/100,1000,300)
profit_l <- c()
for(i in 1:10000) {
profit_l <- c(profit_l,min(rnorm(1,1000,300),Q_l)*100-Q_l*43)
}
mean(profit_l)
Q_l <- qnorm((100-43)/100,1000,300)
profit_l <- c()
for(i in 1:10000) {
profit_l <- c(profit_l,min(rnorm(1,1000,300),Q_l)*100-Q_l*43)
}
mean(profit_l)
Q_l <- qnorm((100-43)/100,1000,300)
profit_l <- c()
for(i in 1:5000) {
profit_l <- c(profit_l,min(rnorm(1,1000,300),Q_l)*100-Q_l*43)
}
mean(profit_l)
Q_h <- qnorm((100-43)/100,1000,300)
profit_h <- c()
for(i in 1:5000) {
profit_h <- c(profit_h,min(rnorm(1,5000,1000),Q_l)*100-Q_l*43)
}
Q_h <- qnorm((100-43)/100,1000,300)
profit_h <- c()
for(i in 1:5000) {
profit_h <- c(profit_h,min(rnorm(1,5000,1000),Q_h)*100-Q_h*43)
}
mean(profit_h)
Q_h <- qnorm((100-43)/100,5000,100)
profit_h <- c()
for(i in 1:5000) {
profit_h <- c(profit_h,min(rnorm(1,5000,1000),Q_h)*100-Q_h*43)
}
mean(profit_h)
x <- runif(1)
p <- exp(x)/(1+exp(x))
x <- runif(1)
p <- runif(1)
rand <- runif(100)
rand <- runif(100) < p
rand <- as.numeric(runif(100) < p)
rand <- as.numeric(runif(100) < p)
which(rand == 1)
min(which(rand == 1))
p <- runif(1)
rand <- as.numeric(runif(100) > p)
rand[min(which(rand == 1)):length(rand)] <- 0
p <- runif(1)
rand <- as.numeric(runif(100) > p)
rand[min(which(rand == 0)):length(rand)] <- 0
rand
p <- runif(1)
p <- runif(1)
rand <- runif(100)
rand <- as.numeric(rand > p)
rand <- runif(100)
rand <- runif(100)
rand <- runif(100)
rand <- as.numeric(rand < p)
rand[min(which(rand == 0)):length(rand)] <- 0
rand
rand <- rnorm(10000,1000,250)
rand <- rand/2
mean(rand)
sd(rand)
rand <- rnorm(20000,1000,250)
sd(rand)
rand <- rand/2
sd(rand)
demand <- rnorm(1000,890,189)
Q_star <- 1032
demand <- rnorm(1000,890,189)
Q_star <- 1032
demand <- rnorm(1000,890,189)
profit <- function(d,Q_star,Co,Cu)
profit <- function(d,Q_star,Co,Cu) {
d <- max(d,0)
if(d > Q_star) {
return(Cu)
} else {
return(Co)
}
}
Q_star <<- 1032
demand <<- rnorm(1000,890,189)
Co <<- 25
Cu <<- 85
profit <- function(d) {
d <- max(d,0)
if(d > Q_star) {
return(Cu)
} else {
return(Co)
}
}
rtn <- unlink(lapply(demand, profit))
summary(rtn)
rtn
d
demand
Q_star
rtn <- unlink(sapply(demand, profit))
summary(rtn)
rtn <- unlink(lapply(demand, profit))
summary(rtn)
profit(1200)
profit(1800)
profit(100)
profit <- function(d) {
d <- max(d,0)
if(d > Q_star) {
return(Cu*(d-Q_star))
} else {
return(Co*(Q_star-d))
}
}
rtn <- unlink(lapply(demand, profit))
summary(rtn)
rtn <- unlist(lapply(demand, profit))
summary(rtn)
profit(940)
demand[1] <- 920
demand[1] <- 940
i = 1
if(demand[i] > Q_star) {
profit <- Q_star*(p-c) + (demand[i]-Q_star)*pi
} else {
profit <- demand[i]*(p-c) + (Q_star-demand[i])*(s-c)
}
Q_star <- 1032
Co <- 25
Cu <- 85
p <- 120
c <- 80
s <- 55
pi <- 45
if(demand[i] > Q_star) {
profit <- Q_star*(p-c) + (demand[i]-Q_star)*pi
} else {
profit <- demand[i]*(p-c) + (Q_star-demand[i])*(s-c)
}
ret <- c()
for(i in 1:length(demand)) {
if(demand[i] > Q_star) {
profit <- Q_star*(p-c) + (demand[i]-Q_star)*pi
} else {
profit <- demand[i]*(p-c) + (Q_star-demand[i])*(s-c)
}
ret <- c(ret,profit)
}
summary(ret)
ret <- c()
for(i in 1:length(demand)) {
demand[i] <- max(demand[i],0)
if(demand[i] > Q_star) {
profit <- Q_star*(p-c) + (demand[i]-Q_star)*pi
} else {
profit <- demand[i]*(p-c) + (Q_star-demand[i])*(s-c)
}
ret <- c(ret,profit)
}
summary(ret)
setwd('/Users/julianregatky/Documents/GitHub/client_churn_ML2020')
load('results.RData')
# Cargamos todas las librerías necesarias
# pacman las carga y, de no estar instaladas, previamente las instala
if (!require('pacman')) install.packages('pacman')
pacman::p_load(tidyverse,mlr,glmnet,ROCR,splines,rpart,randomForest,gbm,tictoc)
acc.lasso <- sum(diag(prop.table(table(test$TARGET == 0,pred.lasso <= sum(train$TARGET)/length(train$TARGET)))))
# Importamos el dataset
dataset <- read.table('train.csv', header = T, sep =',', dec = '.')
# Cargamos funciones propias
source('functions.R')
# Eliminamos ID
dataset <- dataset %>% select(-ID)
# Eliminamos features cuasi-constantes
dataset <- removeConstantFeatures(dataset,
perc = 0.01, # Fijamos threshold del 99%
dont.rm = 'TARGET')
# Unificamos features duplicados
del_col <- c(); dataset_full <- dataset %>% na.omit()
for(i in 1:(ncol(dataset_full)-1)) {
for(j in (i+1):ncol(dataset_full)) {
if(cor(dataset_full[,i],dataset_full[,j]) > 0.99) {
# identificamos columnas idénticas en el data.frame
del_col <- c(del_col,j)
}
}
}
dataset <- dataset[,setdiff(1:ncol(dataset),del_col)] # Eliminamos una de las duplicadas
# La variable 'nac' es la única que cuenta con datos faltantes (56 obs con NA)
# Esto representa sólo ~0.17% de las obs. Imputamos datos faltantes con el valor 2
# que es el reportado para la variable en ~97.8% de los casos.
sum(is.na(dataset$nac))/length(dataset$nac) # ~0.17%
sum(dataset$nac[!is.na(dataset$nac)] == 2)/sum(!is.na(dataset$nac)) # ~97.8%
dataset$nac[is.na(dataset$nac)] <- 2
set.seed(123) # Por replicabilidad
index_train <- sample(1:nrow(dataset),round(nrow(dataset)*0.8)) # Muestra de training
# Separamos en training, validation y testing sets (testing set idem antes)
test <- dataset[setdiff(1:nrow(dataset),index_train),]
index_validation <- sample(index_train,nrow(test)) # Separamos misma cant de obs que test set pero del training set para validación
train <- dataset[setdiff(index_train, index_validation),]
validation <- dataset[index_validation,]
acc.lasso <- sum(diag(prop.table(table(test$TARGET == 0,pred.lasso <= sum(train$TARGET)/length(train$TARGET)))))
acc.rf <- sum(diag(prop.table(table(test$TARGET == 0,pred.rforest <= sum(train$TARGET)/length(train$TARGET)))))
acc.gbm <- sum(diag(prop.table(table(test$TARGET == 0,pred.gbm <= sum(train$TARGET)/length(train$TARGET)))))
raound(acc.lasso,4)
round(acc.lasso,4)
round(acc.rf,4)
round(acc.gbm,4)
rm(list=ls())
# Cargamos todas las librerías necesarias
# pacman las carga y, de no estar instaladas, previamente las instala
if (!require('pacman')) install.packages('pacman')
pacman::p_load(tidyverse,mlr,glmnet,ROCR,splines,rpart,randomForest,gbm,tictoc)
?cv.glmnet
rm(list=ls())
# Cargamos todas las librerías necesarias
# pacman las carga y, de no estar instaladas, previamente las instala
if (!require('pacman')) install.packages('pacman')
pacman::p_load(tidyverse,mlr,glmnet,ROCR,splines,rpart,randomForest,gbm,tictoc)
# Fijamos el working directory
#setwd('/Users/julianregatky/Documents/GitHub/client_churn_ML2020')
setwd('/home/julian/Documents/GitHub/client_churn_ML2020')
# Importamos el dataset
dataset <- read.table('train.csv', header = T, sep =',', dec = '.')
setwd('/Users/julianregatky/Documents/GitHub/client_churn_ML2020')
# Importamos el dataset
dataset <- read.table('train.csv', header = T, sep =',', dec = '.')
# Cargamos funciones propias
source('functions.R')
# Eliminamos ID
dataset <- dataset %>% select(-ID)
# Eliminamos features cuasi-constantes
dataset <- removeConstantFeatures(dataset,
perc = 0.01, # Fijamos threshold del 99%
dont.rm = 'TARGET')
# Unificamos features duplicados
del_col <- c(); dataset_full <- dataset %>% na.omit()
for(i in 1:(ncol(dataset_full)-1)) {
for(j in (i+1):ncol(dataset_full)) {
if(cor(dataset_full[,i],dataset_full[,j]) > 0.99) {
# identificamos columnas idénticas en el data.frame
del_col <- c(del_col,j)
}
}
}
dataset <- dataset[,setdiff(1:ncol(dataset),del_col)] # Eliminamos una de las duplicadas
# La variable 'nac' es la única que cuenta con datos faltantes (56 obs con NA)
# Esto representa sólo ~0.17% de las obs. Imputamos datos faltantes con el valor 2
# que es el reportado para la variable en ~97.8% de los casos.
sum(is.na(dataset$nac))/length(dataset$nac) # ~0.17%
sum(dataset$nac[!is.na(dataset$nac)] == 2)/sum(!is.na(dataset$nac)) # ~97.8%
dataset$nac[is.na(dataset$nac)] <- 2
set.seed(123) # Por replicabilidad
index_train <- sample(1:nrow(dataset),round(nrow(dataset)*0.8)) # Muestra de training
# Si agregamos nomonios adicionales para todos los features
# el algoritmo que usa glmnet para la regresión no converge
# Seleccionamos las variables para splines con un árbol de decisión
# Sólo usamos datos de training para evitar data leakage!
tree <- rpart(formula = TARGET ~., data = dataset[index_train,])
features_importantes <- names(tree$variable.importance)
# Control local polinómico de hasta grado 3
features.spline <- splines_matrix(dataset[,features_importantes])
dataset.lasso <- cbind(dataset,features.spline)
x_train <- dataset.lasso[index_train,] %>% select(-TARGET)
x_test <- dataset.lasso[setdiff(1:nrow(dataset.lasso),index_train),] %>% select(-TARGET)
y_train <- dataset.lasso[index_train,'TARGET']
y_test <- dataset.lasso[setdiff(1:nrow(dataset.lasso),index_train),'TARGET']
# Cross-Validation, 10 folds
x_train_matrix <- model.matrix( ~ .-1, x_train)
cv.out = cv.glmnet(x_train_matrix, y_train, alpha = 1, nfolds = 10, type.measure = 'auc')
cv.out = cv.glmnet(x_train_matrix, factor(y_train), alpha = 1, nfolds = 10, type.measure = 'auc')
class(y_train)
cv.out = cv.glmnet(x_train_matrix, y_train, alpha = 1, nfolds = 10)
plot(cv.out)
lambda_star = cv.out$lambda.min
# Estimamos el modelo con el lambda de CV
model.lasso = glmnet(x = x_train_matrix,
y = y_train,
family = 'binomial',
alpha = 1, # LASSO
lambda = lambda_star,
standardize = TRUE)
?cv.glmnet
cv.out = cv.glmnet(x_train_matrix, y_train, alpha = 1, nfolds = 10, type.measure = "auc")
cv.out = cv.glmnet(x_train_matrix, as.factor(y_train), alpha = 1, nfolds = 10, type.measure = "auc")
