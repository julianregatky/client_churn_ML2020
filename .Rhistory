pred.gbm = predict(model.gbm,newdata=validation)
cat(i,'|',paste(colnames(random_grid),random_grid[i,],collapse = ' - '),'| auc:',performance(prediction(pred.gbm,validation$TARGET),"auc")@y.values[[1]],'\n')
if(performance(prediction(pred.gbm,validation$TARGET),"auc")@y.values[[1]] > best_auc) {
best_model <- model.gbm
best_auc <- performance(prediction(pred.gbm,validation$TARGET),"auc")@y.values[[1]]
}
}
pred.gbm = predict(best_model,newdata=test, type="response")
performance(prediction(pred.gbm,test$TARGET),"auc")@y.values[[1]] #AUC
auc_gbm <- performance(prediction(pred.gbm,test$TARGET),"tpr","fpr")
points(auc_gbm@x.values[[1]],auc_gbm@y.values[[1]], type = 'l', col = 'blue')
full_grid <- expand.grid(mtry = 5:20, sample = seq(0.4,0.8,0.1), maxnodes = 20:50, nodesize = 50:200, ntree = seq(500,1500,100))
random_grid <- full_grid[sample(1:nrow(full_grid),30),]
best_auc <- 0
for(i in 1:nrow(random_grid)) {
random.forest <- randomForest(TARGET ~ .,
data = train,
mtry = random_grid$mtry[i],
ntree = random_grid$ntree[i],
sample = floor(random_grid$sample[i]*nrow(train)),
maxnodes = random_grid$maxnodes[i],
nodesize = random_grid$nodesize[i],
importance = T,
proximity = F
)
pred.rforest = predict(random.forest,newdata=validation)
cat(i,'|',paste(colnames(random_grid),random_grid[i,],collapse = ' - '),'| auc:',performance(prediction(pred.rforest,validation$TARGET),"auc")@y.values[[1]],'\n')
if(performance(prediction(pred.rforest,validation$TARGET),"auc")@y.values[[1]] > best_auc) {
best_model <- random.forest
best_auc <- performance(prediction(pred.rforest,validation$TARGET),"auc")@y.values[[1]]
}
}
pred.rforest = predict(best_model,newdata=test)
performance(prediction(pred.rforest,test$TARGET),"auc")@y.values[[1]] #AUC
auc_rforest <- performance(prediction(pred.rforest,test$TARGET),"tpr","fpr")
points(auc_rforest@x.values[[1]],auc_rforest@y.values[[1]], type = 'l', col = 'red')
# Si agregamos nomonios adicionales para todos los features
# el algoritmo que usa glmnet para la regresión no converge
# Seleccionamos las variables para splines con un árbol de decisión
# Sólo usamos datos de training para evitar data leakage!
tree <- rpart(formula = TARGET ~., data = dataset[index_train,])
features_importantes <- names(tree$variable.importance)
# Control local polinómico de hasta grado 3
features.spline <- splines_matrix(dataset[,features_importantes])
dataset.lasso <- cbind(dataset,features.spline)
x_train <- dataset.lasso[index_train,] %>% select(-TARGET)
x_test <- dataset.lasso[setdiff(1:nrow(dataset.lasso),index_train),] %>% select(-TARGET)
y_train <- dataset.lasso[index_train,'TARGET']
y_test <- dataset.lasso[setdiff(1:nrow(dataset.lasso),index_train),'TARGET']
# Cross-Validation, 10 folds
x_train_matrix <- model.matrix( ~ .-1, x_train)
cv.out = cv.glmnet(x_train_matrix, y_train, alpha = 1, nfolds = 10)
plot(cv.out)
lambda_star = cv.out$lambda.min
# Estimamos el modelo con el lambda de CV
model.lasso = glmnet(x = x_train_matrix,
y = y_train,
family = 'binomial',
alpha = 1, # LASSO
lambda = lambda_star,
standardize = TRUE)
x_test_matrix <- model.matrix( ~ .-1, x_test)
pred.lasso = predict(model.lasso, s = lambda_star , newx = x_test_matrix, type = 'response')
performance(prediction(pred.lasso,y_test),"auc")@y.values[[1]] #AUC
# Si agregamos nomonios adicionales para todos los features
# el algoritmo que usa glmnet para la regresión no converge
# Seleccionamos las variables para splines con un árbol de decisión
# Sólo usamos datos de training para evitar data leakage!
tree <- rpart(formula = TARGET ~., data = dataset[index_train,])
features_importantes <- names(tree$variable.importance)
# Control local polinómico de hasta grado 3
features.spline <- splines_matrix(dataset[,features_importantes])
# Cargamos funciones propias
source('functions.R')
# Control local polinómico de hasta grado 3
features.spline <- splines_matrix(dataset[,features_importantes])
dataset.lasso <- cbind(dataset,features.spline)
x_train <- dataset.lasso[index_train,] %>% select(-TARGET)
x_test <- dataset.lasso[setdiff(1:nrow(dataset.lasso),index_train),] %>% select(-TARGET)
y_train <- dataset.lasso[index_train,'TARGET']
y_test <- dataset.lasso[setdiff(1:nrow(dataset.lasso),index_train),'TARGET']
# Cross-Validation, 10 folds
x_train_matrix <- model.matrix( ~ .-1, x_train)
cv.out = cv.glmnet(x_train_matrix, y_train, alpha = 1, nfolds = 10)
plot(cv.out)
lambda_star = cv.out$lambda.min
# Estimamos el modelo con el lambda de CV
model.lasso = glmnet(x = x_train_matrix,
y = y_train,
family = 'binomial',
alpha = 1, # LASSO
lambda = lambda_star,
standardize = TRUE)
x_test_matrix <- model.matrix( ~ .-1, x_test)
pred.lasso = predict(model.lasso, s = lambda_star , newx = x_test_matrix, type = 'response')
performance(prediction(pred.lasso,y_test),"auc")@y.values[[1]] #AUC
features.spline <- splines_matrix(dataset)
dataset.lasso <- cbind(dataset,features.spline)
x_train <- dataset.lasso[index_train,] %>% select(-TARGET)
x_test <- dataset.lasso[setdiff(1:nrow(dataset.lasso),index_train),] %>% select(-TARGET)
y_train <- dataset.lasso[index_train,'TARGET']
y_test <- dataset.lasso[setdiff(1:nrow(dataset.lasso),index_train),'TARGET']
# Cross-Validation, 10 folds
x_train_matrix <- model.matrix( ~ .-1, x_train)
cv.out = cv.glmnet(x_train_matrix, y_train, alpha = 1, nfolds = 10)
plot(cv.out)
lambda_star = cv.out$lambda.min
# Estimamos el modelo con el lambda de CV
model.lasso = glmnet(x = x_train_matrix,
y = y_train,
family = 'binomial',
alpha = 1, # LASSO
lambda = lambda_star,
standardize = TRUE)
features_importantes
features.spline <- polynomial(dataset = dataset)
dataset.lasso <- cbind(dataset,features.spline)
x_train <- dataset.lasso[index_train,] %>% select(-TARGET)
x_test <- dataset.lasso[setdiff(1:nrow(dataset.lasso),index_train),] %>% select(-TARGET)
y_train <- dataset.lasso[index_train,'TARGET']
y_test <- dataset.lasso[setdiff(1:nrow(dataset.lasso),index_train),'TARGET']
# Cross-Validation, 10 folds
x_train_matrix <- model.matrix( ~ .-1, x_train)
cv.out = cv.glmnet(x_train_matrix, y_train, alpha = 1, nfolds = 10)
plot(cv.out)
# Control local polinómico de hasta grado 3
features.spline <- splines_matrix(dataset[,features_importantes])
dataset.lasso <- cbind(dataset,features.spline)
x_train <- dataset.lasso[index_train,] %>% select(-TARGET)
x_test <- dataset.lasso[setdiff(1:nrow(dataset.lasso),index_train),] %>% select(-TARGET)
y_train <- dataset.lasso[index_train,'TARGET']
y_test <- dataset.lasso[setdiff(1:nrow(dataset.lasso),index_train),'TARGET']
# Cross-Validation, 10 folds
x_train_matrix <- model.matrix( ~ .-1, x_train)
cv.out = cv.glmnet(x_train_matrix, y_train, alpha = 1, nfolds = 10)
plot(cv.out)
lambda_star = cv.out$lambda.min
# Estimamos el modelo con el lambda de CV
model.lasso = glmnet(x = x_train_matrix,
y = y_train,
family = 'binomial',
alpha = 1, # LASSO
lambda = lambda_star,
standardize = TRUE)
x_test_matrix <- model.matrix( ~ .-1, x_test)
pred.lasso = predict(model.lasso, s = lambda_star , newx = x_test_matrix, type = 'response')
performance(prediction(pred.lasso,y_test),"auc")@y.values[[1]] #AUC
auc_lasso <- performance(prediction(pred.lasso,y_test),"tpr","fpr")
plot(auc_lasso)
pacman::p_load(tidyverse,mlr,future.apply,glmnet,ROCR,splines,rpart,randomForest,gbm)
full_grid <- expand.grid(mtry = 5:20, sample = seq(0.4,0.8,0.1), maxnodes = 20:50, nodesize = 50:200, ntree = seq(500,1500,100))
random_grid <- full_grid[sample(1:nrow(full_grid),30),]
random_grid
as.list(random_grid)
fn <- function(grid) {
print(grid)
}
future_apply(random_grid,fn)
?future_apply
future_apply(random_grid,1,fn)
fn <- function(grid) {
print(grid$mtry)
}
future_apply(random_grid,1,fn)
fn <- function(grid) {
print(grid['mtry'])
}
future_apply(random_grid,1,fn)
ct <<- 0
fn <- function(grid) {
random.forest <- randomForest(TARGET ~ .,
data = train,
mtry = grid['mtry'],
ntree = grid['ntree'],
sample = floor(grid['sample']*nrow(train)),
maxnodes = grid['maxnodes'],
nodesize = grid['nodesize'],
importance = T,
proximity = F
)
ct <- ct + 1
print(ct)
return(random.forest)
}
future_apply(random_grid,1,fn)
random_grid <- as.list(t(random_grid))
random_grid
ct <<- 0
fn <- function(grid) {
random.forest <- randomForest(TARGET ~ .,
data = train,
mtry = grid[1],
ntree = grid[2],
sample = floor(grid[3]*nrow(train)),
maxnodes = grid[4]
nodesize = grid[5],
importance = T,
proximity = F
)
ct <- ct + 1
print(ct)
return(random.forest)
}
ct <<- 0
fn <- function(grid) {
random.forest <- randomForest(TARGET ~ .,
data = train,
mtry = grid[1],
ntree = grid[2],
sample = floor(grid[3]*nrow(train)),
maxnodes = grid[4]
nodesize = grid[5],
importance = T,
proximity = F
)
ct <- ct + 1
print(ct)
return(random.forest)
}
ct <<- 0
fn <- function(grid) {
random.forest <- randomForest(TARGET ~ .,
data = train,
mtry = grid[1],
ntree = grid[2],
sample = floor(grid[3]*nrow(train)),
maxnodes = grid[4],
nodesize = grid[5],
importance = T,
proximity = F
)
ct <- ct + 1
print(ct)
return(random.forest)
}
future_lapply(random_grid,1,fn)
future_lapply(random_grid,fn)
random_grid <- full_grid[sample(1:nrow(full_grid),30),]
grid <- as.list(random_grid)
grid
grid <- as.list(t(random_grid))
grid[[1]]
grid[1]
t(random_grid)
grid <- apply(random_grid, 1, as.list)
grid
grid[[1]]
grid[[1]]$mtry
ct <<- 0
fn <- function(grid) {
random.forest <- randomForest(TARGET ~ .,
data = train,
mtry = grid$mtry,
ntree = grid$ntree,
sample = floor(grid$sample*nrow(train)),
maxnodes = grid$maxnodes,
nodesize = grid$nodesize,
importance = T,
proximity = F
)
ct <- ct + 1
print(ct)
return(random.forest)
}
grid <- apply(random_grid, 1, as.list)
grid[[2]]
future_lapply(random_grid,fn)
ct <<- 0
fn <- function(grid) {
random.forest <- randomForest(TARGET ~ .,
data = train,
mtry = grid['mtry'],
ntree = grid['ntree'],
sample = floor(grid['sample']*nrow(train)),
maxnodes = grid['maxnodes'],
nodesize = grid['nodesize'],
importance = T,
proximity = F
)
ct <- ct + 1
print(ct)
return(random.forest)
}
grid <- apply(random_grid, 1, as.list)
future_lapply(random_grid,fn)
fn <- function(grid) {
print(grid)
# random.forest <- randomForest(TARGET ~ .,
#                               data = train,
#                               mtry = grid['mtry'],
#                               ntree = grid['ntree'],
#                               sample = floor(grid['sample']*nrow(train)),
#                               maxnodes = grid['maxnodes'],
#                               nodesize = grid['nodesize'],
#                               importance = T,
#                               proximity = F
# )
# ct <- ct + 1
# print(ct)
# return(random.forest)
}
grid <- apply(random_grid, 1, as.list)
future_lapply(random_grid,fn)
fn <- function(grid) {
print(grid)
print('\n')
# random.forest <- randomForest(TARGET ~ .,
#                               data = train,
#                               mtry = grid['mtry'],
#                               ntree = grid['ntree'],
#                               sample = floor(grid['sample']*nrow(train)),
#                               maxnodes = grid['maxnodes'],
#                               nodesize = grid['nodesize'],
#                               importance = T,
#                               proximity = F
# )
# ct <- ct + 1
# print(ct)
# return(random.forest)
}
grid <- apply(random_grid, 1, as.list)
future_lapply(random_grid,fn)
grid <- apply(random_grid, 2, as.list)
future_lapply(random_grid,fn)
grid
grid[[1]][1]
grid[[1]][2]
grid[[1]][3]
grid <- apply(random_grid, 2, as.list)
grid[[1]][3]
grid[[1]][1]
grid[[1]][2]
grid.list <- list(); for(i in 1:nrow(random_grid)) { grid.list[[i]] <- random_grid[i,] }
grid.list
ct <<- 0
fn <- function(grid) {
random.forest <- randomForest(TARGET ~ .,
data = train,
mtry = grid['mtry'],
ntree = grid['ntree'],
sample = floor(grid['sample']*nrow(train)),
maxnodes = grid['maxnodes'],
nodesize = grid['nodesize'],
importance = T,
proximity = F
)
ct <- ct + 1
print(ct)
return(random.forest)
}
grid.list <- list(); for(i in 1:nrow(random_grid)) { grid.list[[i]] <- random_grid[i,] }
future_lapply(grid.list,fn)
ct <<- 0
fn <- function(grid) {
# random.forest <- randomForest(TARGET ~ .,
#                               data = train,
#                               mtry = grid['mtry'],
#                               ntree = grid['ntree'],
#                               sample = floor(grid['sample']*nrow(train)),
#                               maxnodes = grid['maxnodes'],
#                               nodesize = grid['nodesize'],
#                               importance = T,
#                               proximity = F
# )
# ct <- ct + 1
# print(ct)
# return(random.forest)
print(grid)
}
grid.list <- list(); for(i in 1:nrow(random_grid)) { grid.list[[i]] <- random_grid[i,] }
future_lapply(grid.list,fn)
fn <- function(grid) {
# random.forest <- randomForest(TARGET ~ .,
#                               data = train,
#                               mtry = grid['mtry'],
#                               ntree = grid['ntree'],
#                               sample = floor(grid['sample']*nrow(train)),
#                               maxnodes = grid['maxnodes'],
#                               nodesize = grid['nodesize'],
#                               importance = T,
#                               proximity = F
# )
# ct <- ct + 1
# print(ct)
# return(random.forest)
print(grid$mtry)
}
grid.list <- list(); for(i in 1:nrow(random_grid)) { grid.list[[i]] <- random_grid[i,] }
future_lapply(grid.list,fn)
ct <<- 0
fn <- function(grid) {
random.forest <- randomForest(TARGET ~ .,
data = train,
mtry = grid[1,'mtry'],
ntree = grid[1,'ntree'],
sample = floor(grid[1,'sample']*nrow(train)),
maxnodes = grid[,1'maxnodes'],
nodesize = grid[,1'nodesize'],
importance = T,
proximity = F
)
ct <- ct + 1
print(ct)
return(random.forest)
}
grid.list <- list(); for(i in 1:nrow(random_grid)) { grid.list[[i]] <- random_grid[i,] }
future_lapply(grid.list,fn)
ct <<- 0
fn <- function(grid) {
random.forest <- randomForest(TARGET ~ .,
data = train,
mtry = grid[1,'mtry'],
ntree = grid[1,'ntree'],
sample = floor(grid[1,'sample']*nrow(train)),
maxnodes = grid[,1'maxnodes'],
nodesize = grid[,1'nodesize'],
importance = T,
proximity = F
)
ct <- ct + 1
print(ct)
return(random.forest)
}
ct <<- 0
fn <- function(grid) {
random.forest <- randomForest(TARGET ~ .,
data = train,
mtry = grid[1,'mtry'],
ntree = grid[1,'ntree'],
sample = floor(grid[1,'sample']*nrow(train)),
maxnodes = grid[1,'maxnodes'],
nodesize = grid[1,'nodesize'],
importance = T,
proximity = F
)
ct <- ct + 1
print(ct)
return(random.forest)
}
grid.list <- list(); for(i in 1:nrow(random_grid)) { grid.list[[i]] <- random_grid[i,] }
future_lapply(grid.list,fn)
library(furrr)
r <- future_map_dfr(grid.list,fn)
slow_square <-
function(x = 1) {
x_sq <- x^2
df <- tibble(value=x, value_squared=x_sq)
Sys.sleep(2)
return(df)
}
future_map_dfr(rep(1:12,100), slow_square)
future_map_dfr(1:12, slow_square)
bootstrp <-
function(i) {
^# Sample the data
sample_data <- sample_n(our_data, size = 1e4, replace = T)
^# Run the regression on our sampled data and extract the extract the x coefficient.
x_coef <- lm(y ~ x, data = sample_data)$coef[2]
^# Return value
return(tibble(x_coef = x_coef))
}
bootstrp <-function(i) {
# Sample the data
sample_data <- sample_n(our_data, size = 1e4, replace = T)
# Run the regression on our sampled data and extract the extract the x coefficient.
x_coef <- lm(y ~ x, data = sample_data)$coef[2]
# Return value
return(tibble(x_coef = x_coef))
}
lapply(1:1e4, bootstrp) %>% bind_rows()
our_data <-
tibble(x = rnorm(n), e = rnorm(n)) %>%
mutate(y = 3 + 2*x + e)
our_data <-
tibble(x = rnorm(1e6), e = rnorm(n)) %>%
mutate(y = 3 + 2*x + e)
our_data <-
tibble(x = rnorm(1e6L), e = rnorm(n)) %>%
mutate(y = 3 + 2*x + e)
n = 1e6L
our_data <-
tibble(x = rnorm(n), e = rnorm(n)) %>%
mutate(y = 3 + 2*x + e)
sim_serial <- lapply(1:1e4, bootstrp) %>% bind_rows()
sim_future <- future_lapply(1:1e4, bootstrp, future.seed=123L) %>% bind_rows()
if (!require("pacman")) install.packages("pacman")
pacman:p_load(tictoc, parallel, pbapply, future, furrr)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tictoc, parallel, pbapply, future, furrr)
sim_future <- future_lapply(1:1e4, bootstrp, future.seed=123L) %>% bind_rows()
plan(multiprocess)
sim_future <- future_lapply(1:1e4, bootstrp, future.seed=123L) %>% bind_rows()
gc()
ct <<- 0
fn <- function(grid) {
random.forest <- randomForest(TARGET ~ .,
data = train,
mtry = grid[1,'mtry'],
ntree = grid[1,'ntree'],
sample = floor(grid[1,'sample']*nrow(train)),
maxnodes = grid[1,'maxnodes'],
nodesize = grid[1,'nodesize'],
importance = T,
proximity = F
)
ct <- ct + 1
print(ct)
return(random.forest)
}
grid.list <- list(); for(i in 1:nrow(random_grid)) { grid.list[[i]] <- random_grid[i,] }
r <- future_lapply(grid.list,fn)
r
